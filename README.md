# ä¸ªäººçŸ¥è¯†åº“æ™ºèƒ½åŠ©æ‰‹

åŸºäº LangChain + FastAPI + Milvus + MongoDB æ„å»ºçš„ RAG æ™ºèƒ½åŠ©æ‰‹ï¼Œæ”¯æŒé•¿æœŸè®°å¿†å’Œä¸ªæ€§åŒ–å¯¹è¯ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ” **RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ**ï¼šä½¿ç”¨ Milvus å‘é‡æ•°æ®åº“å®ç°è¯­ä¹‰æ£€ç´¢
- ğŸ§  **é•¿æœŸè®°å¿†**ï¼šä½¿ç”¨ MongoDB å­˜å‚¨ç”¨æˆ·è®°å¿†å’Œå¯¹è¯å†å²
- ğŸ’¬ **ä¸ªæ€§åŒ–å¯¹è¯**ï¼šåŸºäºç”¨æˆ·å†å²è®°å¿†æä¾›ä¸ªæ€§åŒ–å›å¤
- ğŸ“š **çŸ¥è¯†åº“ç®¡ç†**ï¼šæ”¯æŒæ·»åŠ å’Œç®¡ç†æ–‡æ¡£çŸ¥è¯†åº“
- ğŸ”„ **å¯æŒç»­å­¦ä¹ **ï¼šè‡ªåŠ¨ä¿å­˜å¯¹è¯å’Œç”¨æˆ·è®°å¿†ï¼Œå®ç°æŒç»­å­¦ä¹ 
- ğŸš€ **å¤š LLM æ”¯æŒ**ï¼šæ”¯æŒ OpenAIã€DashScopeï¼ˆé€šä¹‰åƒé—®ï¼‰å’Œæœ¬åœ°æ¨¡å‹

## æŠ€æœ¯æ ˆ

- **åç«¯æ¡†æ¶**: FastAPI
- **LLM æ¡†æ¶**: LangChain
- **LLM æä¾›å•†**: OpenAI / DashScopeï¼ˆé€šä¹‰åƒé—®ï¼‰/ æœ¬åœ°æ¨¡å‹ï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰
- **å‘é‡æ•°æ®åº“**: Milvus
- **æ–‡æ¡£æ•°æ®åº“**: MongoDB
- **åµŒå…¥æ¨¡å‹**: Sentence Transformers
- **å‰ç«¯**: React + TypeScript + Vite

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.12+
- Node.js 18+ (å‰ç«¯)
- Docker & Docker Compose (æ•°æ®åº“æœåŠ¡)
- uv (Python åŒ…ç®¡ç†å™¨ï¼Œæ¨è) æˆ– pip

### 1. å®‰è£…åç«¯ä¾èµ–
h
# ä½¿ç”¨ uv (æ¨è)
uv sync

# æˆ–ä½¿ç”¨ pip
pip install -r requirements.txt### 2. å®‰è£…å‰ç«¯ä¾èµ–

cd frontend
npm install### 3. é…ç½®

é¡¹ç›®æ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼š

#### æ–¹å¼ä¸€ï¼šä½¿ç”¨ YAML é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰

ç¼–è¾‘ `config/settings.yaml`ï¼š

# API é…ç½®
api:
  host: "127.0.0.1"
  port: 8001
  title: "ä¸ªäººçŸ¥è¯†åº“æ™ºèƒ½åŠ©æ‰‹"

# LLM é…ç½®
llm:
  provider: "local"  # "openai" | "dashscope" | "local"
  model: "/root/large_model_project/models/Qwen2.5-3B-Instruct"  # æœ¬åœ°æ¨¡å‹è·¯å¾„
  temperature: 0.7
  max_tokens: 1024
  api_key: "none"  # æœ¬åœ°æ¨¡å‹å¯ä¸ºä»»æ„å€¼
  base_url: "http://localhost:8000/v1"  # æœ¬åœ°æ¨¡å‹æœåŠ¡åœ°å€#### æ–¹å¼äºŒï¼šä½¿ç”¨ç¯å¢ƒå˜é‡

åˆ›å»º `.env` æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰ï¼š

# API é…ç½®
API_HOST=127.0.0.1
API_PORT=8001

# LLM é…ç½®
LLM_PROVIDER=local  # openai | dashscope | local
LLM_MODEL=/root/large_model_project/models/Qwen2.5-3B-Instruct
LLM_BASE_URL=http://localhost:8000/v1
OPENAI_API_KEY=none  # æœ¬åœ°æ¨¡å‹å¯ä¸ºä»»æ„å€¼

# æ•°æ®åº“é…ç½®ï¼ˆå¦‚æœä½¿ç”¨ Dockerï¼Œä¼šè‡ªåŠ¨é…ç½®ï¼‰
MILVUS_HOST=localhost
MILVUS_PORT=19530
MONGODB_URI=mongodb://localhost:27017**æ³¨æ„**ï¼šYAML é…ç½®ä¼˜å…ˆçº§é«˜äºç¯å¢ƒå˜é‡ï¼Œå¦‚æœä¸¤è€…éƒ½å­˜åœ¨ï¼ŒYAML é…ç½®ä¼šè¦†ç›–ç¯å¢ƒå˜é‡ã€‚

### 4. å¯åŠ¨æ•°æ®åº“æœåŠ¡

ä½¿ç”¨ Docker Compose å¯åŠ¨ Milvus å’Œ MongoDBï¼š

# å¯åŠ¨æ•°æ®åº“æœåŠ¡ï¼ˆMilvusã€etcdã€MinIOã€MongoDBï¼‰
docker-compose up -d milvus etcd minio mongodb

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f milvus
docker-compose logs -f mongodb### 5. å¯åŠ¨åç«¯åº”ç”¨

# æ–¹å¼ä¸€ï¼šä½¿ç”¨ uv
uv run python main.py

# æ–¹å¼äºŒï¼šä½¿ç”¨ Python
python main.py

# æ–¹å¼ä¸‰ï¼šä½¿ç”¨ uvicorn
uvicorn main:app --host 127.0.0.1 --port 8001 --reloadåç«¯åº”ç”¨å°†åœ¨ http://127.0.0.1:8001 å¯åŠ¨ã€‚

### 6. å¯åŠ¨å‰ç«¯åº”ç”¨

cd frontend
npm run devå‰ç«¯åº”ç”¨å°†åœ¨ http://localhost:3000 å¯åŠ¨ã€‚

### 7. å¼€å‘ç¯å¢ƒå¿«é€Ÿå¯åŠ¨ï¼ˆWindows PowerShellï¼‰

é¡¹ç›®æä¾›äº†å¼€å‘ç¯å¢ƒå¯åŠ¨è„šæœ¬ï¼Œå¯ä»¥ä¸€é”®å¯åŠ¨æ‰€æœ‰æœåŠ¡ï¼š
l
.\start-dev.ps1è¯¥è„šæœ¬ä¼šï¼š
- æ£€æŸ¥å¹¶å¯åŠ¨ Docker æ•°æ®åº“æœåŠ¡
- å¯åŠ¨åç«¯æœåŠ¡ï¼ˆç«¯å£ 8001ï¼‰
- å¯åŠ¨å‰ç«¯æœåŠ¡ï¼ˆç«¯å£ 3000ï¼‰

### 8. è®¿é—®åº”ç”¨

- **å‰ç«¯ç•Œé¢**: http://localhost:3000
- **API æ–‡æ¡£**: http://127.0.0.1:8001/docs
- **API æ ¹è·¯å¾„**: http://127.0.0.1:8001/

## LLM é…ç½®è¯´æ˜

é¡¹ç›®æ”¯æŒä¸‰ç§ LLM æä¾›å•†ï¼š

### OpenAI

llm:
  provider: "openai"
  model: "gpt-4-turbo-preview"  # æˆ– gpt-3.5-turbo
  temperature: 0.7
  max_tokens: 2000
  api_key: "sk-your-openai-api-key"æˆ–åœ¨ `.env` æ–‡ä»¶ä¸­ï¼š
LLM_PROVIDER=openai
LLM_MODEL=gpt-4-turbo-preview
OPENAI_API_KEY=sk-your-openai-api-key### DashScopeï¼ˆé€šä¹‰åƒé—®ï¼‰

llm:
  provider: "dashscope"
  model: "qwen-plus"  # qwen-turbo | qwen-plus | qwen-max
  temperature: 0.7
  max_tokens: 2000
  api_key: "sk-your-dashscope-api-key"æˆ–åœ¨ `.env` æ–‡ä»¶ä¸­ï¼š
LLM_PROVIDER=dashscope
LLM_MODEL=qwen-plus
OPENAI_API_KEY=sk-your-dashscope-api-key  # DashScope API Key**è·å– DashScope API Key**: https://dashscope.console.aliyun.com/

### æœ¬åœ°æ¨¡å‹ï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰

llm:
  provider: "local"
  model: "/path/to/your/model"  # æ¨¡å‹è·¯å¾„æˆ–æ ‡è¯†ç¬¦
  temperature: 0.7
  max_tokens: 1024  # æ ¹æ®æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦è°ƒæ•´
  api_key: "none"  # å¯ä¸ºä»»æ„å€¼
  base_url: "http://localhost:8000/v1"  # æœ¬åœ°æ¨¡å‹æœåŠ¡åœ°å€æˆ–åœ¨ `.env` æ–‡ä»¶ä¸­ï¼š
LLM_PROVIDER=local
LLM_MODEL=/path/to/your/model
LLM_BASE_URL=http://localhost:8000/v1
OPENAI_API_KEY=none**æ³¨æ„**ï¼š
- æœ¬åœ°æ¨¡å‹æœåŠ¡éœ€è¦æä¾› OpenAI å…¼å®¹çš„ API æ¥å£
- `base_url` åº”è¯¥æŒ‡å‘æœ¬åœ°æ¨¡å‹æœåŠ¡çš„ `/v1` ç«¯ç‚¹
- `max_tokens` éœ€è¦æ ¹æ®æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦è°ƒæ•´ï¼ˆå¦‚ Qwen2.5-3B ä¸º 2048ï¼‰

## API ä½¿ç”¨ç¤ºä¾‹

### èŠå¤©

curl -X POST "http://127.0.0.1:8001/api/v1/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user123",
    "message": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "use_memory": true,
    "use_rag": true
  }'### æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“

curl -X POST "http://127.0.0.1:8001/api/v1/documents" \
  -H "Content-Type: application/json" \
  -d '{
    "texts": [
      "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
      "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œé€šè¿‡ç®—æ³•ä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚"
    ],
    "metadatas": [
      {"source": "wiki"},
      {"source": "wiki"}
    ]
  }'### æ·»åŠ ç”¨æˆ·è®°å¿†

curl -X POST "http://127.0.0.1:8001/api/v1/memories" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user123",
    "content": "ç”¨æˆ·å–œæ¬¢å–å’–å•¡ï¼Œæ¯å¤©æ—©ä¸Šéƒ½ä¼šå–ä¸€æ¯",
    "memory_type": "preference",
    "importance": 0.8
  }'### è·å–ç”¨æˆ·è®°å¿†

curl -X GET "http://127.0.0.1:8001/api/v1/memories/user123"### å¥åº·æ£€æŸ¥

curl -X GET "http://127.0.0.1:8001/api/v1/health"## é¡¹ç›®ç»“æ„
